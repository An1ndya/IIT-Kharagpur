{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/An1ndya/IIT-Kharagpur/blob/main/Copy_of_COCO_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uinvHUWOr1pt"
      },
      "source": [
        "**Download and process COCO dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5akh_qrqjPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396f7448-dacb-4908-dad8-031b812dce64"
      },
      "source": [
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "#Removes old data files\n",
        "cleanBeforeProcessing = True\n",
        "#Image dimensions\n",
        "imgSize = 256\n",
        "\n",
        "#Download image dataset\n",
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "#Extract archive \n",
        "!unzip train2017.zip\n",
        "#Clean up\n",
        "!rm train2017.zip\n",
        "\n",
        "#Resize all images\n",
        "for root, dirs, files in os.walk(\"train2017\"):\n",
        "  for fname in files:\n",
        "    if \".png\" in fname or \".jpg\" in fname:\n",
        "      path = os.path.join(root, fname)\n",
        "      im = Image.open(path)\n",
        "      im_resized = im.resize((imgSize, imgSize))\n",
        "      im_resized.save(path)\n",
        "      print(\"Resizing: \" + fname)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 05:00:10--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.36.20\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.36.20|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘train2017.zip’\n",
            "\n",
            "train2017.zip         1%[                    ] 190.58M  12.7MB/s    eta 26m 41s^C\n",
            "Archive:  train2017.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of train2017.zip or\n",
            "        train2017.zip.zip, and cannot find train2017.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanBeforeProcessing = True\n",
        "#Download captions\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "#Extract archive \n",
        "!unzip annotations_trainval2017.zip\n"
      ],
      "metadata": {
        "id": "4q60J44HSrCE",
        "outputId": "758fc788-189a-4721-bb9a-662d956d7578",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-11 20:20:46--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.169.73\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.169.73|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip.2’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  33.7MB/s    in 7.8s    \n",
            "\n",
            "2022-11-11 20:20:54 (31.0 MB/s) - ‘annotations_trainval2017.zip.2’ saved [252907541/252907541]\n",
            "\n",
            "Archive:  annotations_trainval2017.zip\n",
            "replace annotations/instances_train2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "if cleanBeforeProcessing:\n",
        "  os.system('rm od-labelsonly.txt')\n",
        "  os.system('rm od-labelsimgid.txt')\n",
        "\n",
        "#Read images information\n",
        "with open('annotations/instances_train2017.json', 'r') as f:\n",
        "    array = json.load(f)\n",
        "\n",
        "#Read all images paths\n",
        "imagePaths = []\n",
        "for root, dirs, files in os.walk(\"train2017\"):\n",
        "  for fname in files:\n",
        "    if \".png\" in fname or \".jpg\" in fname:\n",
        "      path = os.path.join(root, fname)\n",
        "      imagePaths.append(path)\n"
      ],
      "metadata": {
        "id": "S5be4vFeTVZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Read information for each image\n",
        "idAlreadyProcessed = {}\n",
        "count = 0\n",
        "for info in array['annotations']:\n",
        "\n",
        "  image_id = int(info['id'])\n",
        "  temp = \"000000000000\"\n",
        "  image_id_str = temp[:len(temp)-len(str(image_id))] + str(image_id)\n",
        "\n",
        "  path = \"train2017/\" + image_id_str + \".jpg\"\n",
        "  label = info['category_id']\n",
        "\n",
        "  #Choose first caption and image_id\n",
        "  with open(\"od-labelsonly.txt\", 'a') as out:\n",
        "    out.write(str(label) + '\\n')\n",
        "  #Write this as CSV ... it will be easier to access later on\n",
        "  with open(\"od-labelsimgid.txt\", 'a') as out:\n",
        "    out.write(str(image_id) + \" : \" + str(label) + '\\n')\n",
        "\n",
        "  count += 1\n",
        "  if count%5000 == 0:\n",
        "    print(\"Processed \" + str(count))"
      ],
      "metadata": {
        "id": "IEQkxJ1nTqa6",
        "outputId": "fc02e6ca-6415-484e-85bc-2212419055e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 5000\n",
            "Processed 10000\n",
            "Processed 15000\n",
            "Processed 20000\n",
            "Processed 25000\n",
            "Processed 30000\n",
            "Processed 35000\n",
            "Processed 40000\n",
            "Processed 45000\n",
            "Processed 50000\n",
            "Processed 55000\n",
            "Processed 60000\n",
            "Processed 65000\n",
            "Processed 70000\n",
            "Processed 75000\n",
            "Processed 80000\n",
            "Processed 85000\n",
            "Processed 90000\n",
            "Processed 95000\n",
            "Processed 100000\n",
            "Processed 105000\n",
            "Processed 110000\n",
            "Processed 115000\n",
            "Processed 120000\n",
            "Processed 125000\n",
            "Processed 130000\n",
            "Processed 135000\n",
            "Processed 140000\n",
            "Processed 145000\n",
            "Processed 150000\n",
            "Processed 155000\n",
            "Processed 160000\n",
            "Processed 165000\n",
            "Processed 170000\n",
            "Processed 175000\n",
            "Processed 180000\n",
            "Processed 185000\n",
            "Processed 190000\n",
            "Processed 195000\n",
            "Processed 200000\n",
            "Processed 205000\n",
            "Processed 210000\n",
            "Processed 215000\n",
            "Processed 220000\n",
            "Processed 225000\n",
            "Processed 230000\n",
            "Processed 235000\n",
            "Processed 240000\n",
            "Processed 245000\n",
            "Processed 250000\n",
            "Processed 255000\n",
            "Processed 260000\n",
            "Processed 265000\n",
            "Processed 270000\n",
            "Processed 275000\n",
            "Processed 280000\n",
            "Processed 285000\n",
            "Processed 290000\n",
            "Processed 295000\n",
            "Processed 300000\n",
            "Processed 305000\n",
            "Processed 310000\n",
            "Processed 315000\n",
            "Processed 320000\n",
            "Processed 325000\n",
            "Processed 330000\n",
            "Processed 335000\n",
            "Processed 340000\n",
            "Processed 345000\n",
            "Processed 350000\n",
            "Processed 355000\n",
            "Processed 360000\n",
            "Processed 365000\n",
            "Processed 370000\n",
            "Processed 375000\n",
            "Processed 380000\n",
            "Processed 385000\n",
            "Processed 390000\n",
            "Processed 395000\n",
            "Processed 400000\n",
            "Processed 405000\n",
            "Processed 410000\n",
            "Processed 415000\n",
            "Processed 420000\n",
            "Processed 425000\n",
            "Processed 430000\n",
            "Processed 435000\n",
            "Processed 440000\n",
            "Processed 445000\n",
            "Processed 450000\n",
            "Processed 455000\n",
            "Processed 460000\n",
            "Processed 465000\n",
            "Processed 470000\n",
            "Processed 475000\n",
            "Processed 480000\n",
            "Processed 485000\n",
            "Processed 490000\n",
            "Processed 495000\n",
            "Processed 500000\n",
            "Processed 505000\n",
            "Processed 510000\n",
            "Processed 515000\n",
            "Processed 520000\n",
            "Processed 525000\n",
            "Processed 530000\n",
            "Processed 535000\n",
            "Processed 540000\n",
            "Processed 545000\n",
            "Processed 550000\n",
            "Processed 555000\n",
            "Processed 560000\n",
            "Processed 565000\n",
            "Processed 570000\n",
            "Processed 575000\n",
            "Processed 580000\n",
            "Processed 585000\n",
            "Processed 590000\n",
            "Processed 595000\n",
            "Processed 600000\n",
            "Processed 605000\n",
            "Processed 610000\n",
            "Processed 615000\n",
            "Processed 620000\n",
            "Processed 625000\n",
            "Processed 630000\n",
            "Processed 635000\n",
            "Processed 640000\n",
            "Processed 645000\n",
            "Processed 650000\n",
            "Processed 655000\n",
            "Processed 660000\n",
            "Processed 665000\n",
            "Processed 670000\n",
            "Processed 675000\n",
            "Processed 680000\n",
            "Processed 685000\n",
            "Processed 690000\n",
            "Processed 695000\n",
            "Processed 700000\n",
            "Processed 705000\n",
            "Processed 710000\n",
            "Processed 715000\n",
            "Processed 720000\n",
            "Processed 725000\n",
            "Processed 730000\n",
            "Processed 735000\n",
            "Processed 740000\n",
            "Processed 745000\n",
            "Processed 750000\n",
            "Processed 755000\n",
            "Processed 760000\n",
            "Processed 765000\n",
            "Processed 770000\n",
            "Processed 775000\n",
            "Processed 780000\n",
            "Processed 785000\n",
            "Processed 790000\n",
            "Processed 795000\n",
            "Processed 800000\n",
            "Processed 805000\n",
            "Processed 810000\n",
            "Processed 815000\n",
            "Processed 820000\n",
            "Processed 825000\n",
            "Processed 830000\n",
            "Processed 835000\n",
            "Processed 840000\n",
            "Processed 845000\n",
            "Processed 850000\n",
            "Processed 855000\n",
            "Processed 860000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils \n",
        "\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    \"\"\"COCO dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.label = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.label.iloc[idx, 0])\n",
        "        image = io.imread(img_name)\n",
        "        label = self.label.iloc[idx, 1:]\n",
        "        label = np.array([label])\n",
        "        label = label.astype('int32').reshape(-1, 2)\n",
        "        sample = {'image': image, 'label': label}\n",
        "\n",
        "        # if self.transform:\n",
        "        #     sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "KUUgOqlt3HDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = COCODataset(csv_file='od-labelsimgid.txt',\n",
        "                                    root_dir='data/') # replace the locations\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    print(i, sample['image'].shape, sample['label'].shape)\n",
        "\n",
        "    ax = plt.subplot(1, 4, i + 1)\n",
        "    plt.tight_layout()\n",
        "    ax.set_title('Sample #{}'.format(i))\n",
        "    ax.axis('off')\n",
        "    plt.imshow(sample['image'])\n",
        "\n",
        "    if i == 3:\n",
        "        plt.show()\n",
        "        break"
      ],
      "metadata": {
        "id": "Kb4VS55B4WWE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}